\documentclass[a4paper,11pt]{article}
\usepackage{amsmath,amssymb,amsfonts,latexsym}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage[margin=1.1in]{geometry}
\usepackage{url}
\usepackage{verbatim}
\usepackage{fixltx2e}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{listings}
\usepackage{lmodern}
\usepackage[spanish]{babel}
\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\title{Speech compression \\
M\'etodos Num\'ericos Avanzados}
\author{Enzo Altamiranda, Cristian Ontiver, Valeria Serber}
\date{\today}
\begin{document}

\maketitle
\thispagestyle{empty}
\vspace{3cm}

\renewcommand{\abstractname}{Resumen}
\begin{abstract}
El siguiente informe explora el uso de la transformada rápida de Fourier, en
combinación con la codificación de Huffman para la compresión de voz. Se
realiza un estudio comparativo de diferentes niveles de cuantificación y sus
efectos tanto en el tamaño como la calidad - en términos de distorción - del
archivo comprimido.
\end{abstract}
\newpage
\section{Palabras Clave}
Transformada Rápida de Fourier, Compresión de voz, Codificación de Huffman
\newpage
\section{Introducci\'on}
\begin{comment}
[Introduce el tema contextualizando la informacion. Puede incluirse un parrafo
con una breve descripcion historica, otro parrafo motivando el tema. El ultimo
parrafo de esta seccion tiene que ser la descripcion de la estructura del
artıculo, explicitando en que seccion se trata cada tema.]
\end{comment}
\paragraph{}
La voz, entendida como el sonido generado por el aparato fonador humano, se
encuentra en frencuencias de entre $80$ a $1100$ Hz.  Teniendo esto en cuenta,
a la hora de almacenar digitalmente la señal de voz, es posible utilizar
métodos para reducir el tamaño necesario para guardar la información
correspondiente a esta señal.  Tradicionalmente estos se dividen en dos. Por un
lado métodos "lossless" (sin pérdida), es decir, métodos que almacenan la
información de modo tal que esta pueda ser recuperada en su totalidad. Por
otro, métodos "lossy" (con pérdida), en los cuales, tomando ventaja de que el
rango de audición humano suele situarse entre los $20$ y $20.000$ Hz (con
variaciones de individuo a individuo), descartan información considerada
redundante (aquella por encima o por debajo de ese rango).\\
Aquí exploramos una forma de lograr lo segundo, mediante el uso de la
transformada rápida de Fourier, en conjunto con la codificación de Huffman,
para lograr almacenar diferentes archivos de sonidos conteniendo voces, de
manera que se reduzca el tamaño necesario.\\
Como se verá más adelante, una reducción en la distorición y aumento en la
fidelidad del audio comprimido implican la necesidad de mayor información, y
por ende, una menor reducción en relación al tamaño (en bytes) original.
\paragraph{}
\newpage
\section{Metodolog\'ia}
\begin{comment}
[Hay que relatar los pasos que se fueron realizando, incluyendo los modelos
utilizados, los analisis hechos, las pruebas realizadas.]
\end{comment}
\subsection{C\'alculo de la matriz del Modelo de Ising}
\paragraph{}
\subsubsection{Primera Iteraci\'on}
\paragraph{}
En la primera iteraci\'on se opt\'o por almacenar cada matriz en un arreglo de arreglos. Se comenz\'o realizando una implementaci\'on del algoritmo est\'andar de multiplicaci\'on de matrices, debido a la simpleza del c\'odigo correspondiente. Se prob\'o esta versi\'on creando las matrices \emph{K} y \emph{L} y multiplic\'andolas. El orden temporal de este algoritmo es c\'ubico, es decir, \emph{O(n\textsuperscript{3})}. Debido a esto, al calcular matrices pequeñas el algoritmo termina de forma r\'apida, sin embargo, al probar matrices de mayor tamaño, el tiempo crece en gran medida.
\subsubsection{Segunda Iteraci\'on}
\paragraph{}
Debido a los resultados anteriores, se hizo evidente que se deb\'ia mejorar el programa para disminuir su complejidad. Para ello, se utiliz\'o el hecho de que las matrices \emph{K} y \emph{L} son \textbf{ralas}, es decir, matrices de gran tamaño, en la que la mayor\'ia de los elementos son cero. Para optimizar el algoritmo, se opt\'o por cambiar la estructura de datos que almacena las matrices, de modo que los ceros no se almacenen, y el algoritmo pueda saltear las operaciones que den como resultado cero. La estructura utilizada en esta iteraci\'on se conoce como \emph{``Compressed column storage''} \emph{(CCS)}, o alternativamente \emph{``Compressed sparse column''}, y es la representaci\'on tradicional utilizada en \emph{MATLAB} al usar la funci\'on \emph{``sparse''}.
\paragraph{}
Esta representaci\'on cuenta con tres arreglos. El primero, al cual llamaremos \textbf{\emph{values}}, contiene los valores no nulos de la matriz, de tamaño \textbf{\emph{nnz}} (del ingl\'es \emph{``number of nonzeros''}). El segundo, \textbf{\emph{r\textsubscript{i}}}, indica el \'indice de la fila del elemento que se encuentra en el primer arreglo y en la misma posici\'on. Dicho arreglo tambi\'en tiene tamaño \emph{nnz}. El tercero, \textbf{\emph{cp}}, tiene como tamaño la cantidad de columnas m\'as uno, donde en la posici\'on \emph{j} del arreglo se guarda el \'indice en el arreglo de valores, en el que se encuentra el primer elemento no nulo de la columna \emph{j}. El \'ultimo elemento de \emph{cp} es el valor \emph{nnz}. Si alguna columna tuviera todos ceros, en el \'indice de esa columna se coloca el mismo valor que en la pr\'oxima columna. Utilizando el arreglo \emph{cp} se puede conocer la cantidad de elementos no nulos presentes en cada columna \emph{j}, para ello basta con calcular la resta: \emph{cp[j+1] - cp[j]}.\\

\textbf{Ejemplo de almacenamiento de una matriz utilizando Compressed column storage}\\
\[ M = \left( \begin{array}{ccccc}
0 & 3 & 0 & 5 & 7 \\
0 & 0 & 0 & 3 & 8 \\
1 & 0 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 & 5\end{array} \right)\]

\paragraph{}
Notar que la tercer columna no tiene valores. Esto se representa usando el mismo valor en \emph{cp} que en la pr\'oxima columna. Esto es consistente con el hecho de que la cantidad de elementos no nulos presentes en la columna \emph{j} se puede conocer con la resta \[\emph{cp[j+1] - cp[j]}.\]

\[ values = \left[ \begin{array}{cccccccc}
1, & 3, & 2, & 5, & 3, & 7, & 8, & 5 \\\end{array} \right]\]

\[ ri = \left[ \begin{array}{cccccccc}
2, & 0, & 3, & 0, & 1, & 0, & 1, & 3 \\\end{array} \right]\]

\[ cp = \left[ \begin{array}{cccccc}
0, & 1, & 3, & 3, & 5, & 8\\\end{array} \right]\]
\\
\textbf{Complejidad espacial de la representaci\'on CCS}
\paragraph{}
Para una \textbf{matriz densa}, es decir, que no tiene elementos nulos, de \emph{m} x \emph{n}, se necesitan \emph{m} punteros en memoria, cada uno apuntando a los \emph{m} arreglos de \emph{n} elementos. En comparaci\'on con una matriz en representaci\'on \emph{CCS}, se necesita espacio para \emph{nnz$\cdot$sizeof(elems) + (nnz + cols)$\cdot$sizeof(\'indices)}. Para una aproximaci\'on m\'as f\'acil de entender, si suponemos que el tamaño de \'indices, elementos, y punteros en memoria usan todos a misma cantidad de bytes, se tiene que la complejidad espacial de la matriz densa es \emph{m+m$\cdot$n}, es decir, \emph{O(m$\cdot$n)}. Como en el caso del Modelo de Ising las matrices son cuadradas, queda de orden espacial \emph{O(n\textsuperscript{2})}. En cambio, la complejidad espacial de la \textbf{matriz rala} es \emph{2$\cdot$nnz+n}, es decir orden \emph{O(nnz+n)}, que es \emph{lineal}.\\
\\
\textbf{Algoritmo de multiplicaci\'on CCS}
\paragraph{}
\newpage
\section{Resultados}
\subsection{Comparaci\'on entre ambas iteraciones}
\paragraph{}
Se realizaron pruebas en ambas iteraciones para descubrir las diferencias entre ellas. Se utilizaron matrices de diferentes tamaños, y se utiliz\'o el comando \emph{``time''} de \emph{Linux} para medir el tiempo de cada caso. A continuaci\'on se muestra una tabla con los valores que se utilizaron en las pruebas, y luego los gr\'aficos que se realizaron a partir de ellas.\\
\\

\begin{tabular}{|l||l|l||l|}
\hline
\multicolumn{2}{|l|}{Multiplicaci\'on est\'andar}&\multicolumn{2}{l|}{Multiplicaci\'on CCS}\\
\cline{1-4}
\textbf{m}&\textbf{tiempo (seg)}&\textbf{m}&\textbf{tiempo (seg)}\\
\hline\hline
100 & 0.02 & 100 & 0\\
150 & 0.05 & 150 & 0\\
200 & 0.13 & 200 & 0\\
250 & 0.25 & 250 & 0.01\\
300 & 1.32 & 300 & 0.01\\
350 & 2.88 & 350 & 0.01\\
400 & 4.24 & 400 & 0.01\\
450 & 6.51 & 450 & 0.02\\
500 & 8.90 & 500 & 0.02\\
550 & 12.05 & 550 & 0.02\\
600 & 15.81 & 600 & 0.03\\
650 & 20.22 & 650 & 0.03\\
700 & 25.25 & 700 & 0.04\\
750 & 32.36 & 750 & 0.04\\
800 & 36.33 & 800 & 0.05\\
850 & 45.21 & 850 & 0.06\\
900 & 46.49 & 900 & 0.06\\
950 & 54.95 & 950 & 0.06\\
1000 & 62.82 & 1000 & 0.07\\
 &  & 10000 & 3.19\\
 &  & 20000 & 7.14\\
 &  & 25000 & 19.59\\
 &  & 30000 & 28.22\\
 &  & 35000 & 39.30\\
 &  & 40000 & 52.50\\
 &  & 45000 & 63.79\\
\hline
\end{tabular}\\
\\
\small\emph{Tabla 1: Muestra el tiempo en segundos, que tardaron los dos algoritmos implementados respectivamente, a partir de distintos valores de m.}

\subsubsection{Primera iteraci\'on}
\subsubsection{Segunda iteraci\'on}
\subsection{Descripci\'on de los resultados}
\paragraph{}
El resultado m\'as importante que se puede obtener de los gr\'aficos anteriores
es que en el intervalo de 0 a 70 segundos, el primer algoritmo puede resolver
hasta matrices de \emph{m = 1000}, mientras que el segundo puede resolver hasta
matrices de \emph{m = 500000} aproximadamente.  Adem\'as, se puede observar que
la curva del primer algoritmo crece mucho m\'as r\'apido que la segunda. Esto
puede corresponderse con el hecho de que el primer algoritmo es de
\emph{O(n\textsuperscript{3})}, mientras que el otro es de orden menor.

\newpage
\section{Conclusiones}
\subsection{C\'alculo de A}
\paragraph{}
Si bien se logr\'o optimizar bastante el algoritmo que calcula la matriz A,
mientras se realizaban pruebas se pudo observar un patr\'on en la
construcci\'on de A. Se not\'o que a partir de \emph{m = 3}, la matriz \emph{A}
puede escribirse gen\'ericamente, ya que pequeños bloques de elementos se
repiten a lo largo de la estructura, aumentando la cantidad de bloques
l\'ogicamente mientras \emph{m} aumenta. Por lo tanto, se podr\'ia evitar tener
que calcular \emph{A} a partir del producto entre \emph{K} y \emph{L}, y en vez
de eso, seguir la siguiente regla para representarla:

\paragraph{}
\paragraph{}
\paragraph{}
Cuando \emph{m = 3}, la matriz comienza a expandirse dejando ceros en varias
posiciones. Adem\'as puede verse que el bloque del medio compuesto por \emph{Q,
R, S y T} comienza a repetirse.
\paragraph{}
Entonces, para \emph{m \textgreater = 3}, la matriz puede escribirse de forma
gen\'erica de la siguiente manera:

\begin{equation} \label{matrizM4}
A =
\left( \begin{array}{cccccccccccc}
T & Q & R &   &   &  &  &  &  &  &  & S\\
  & S & T & Q & R &  &  &  &  &  &  &  \\
  &   &   & S & T & \ddots&  &  &  &  &  &  \\
  &   &   &   &   &  & \ddots&  &  &  &  &  \\
  &   &   &   &   &  &  & Q & R &  &  &  \\
  &   &   &   &   &  &  & S & T & Q & R &  \\
R &   &   &   &   &  &  &   &   & S & T & Q\\
\end{array} \right)
\end{equation}
\paragraph{}
Siendo
\begin{equation} \label{matrizQ}
Q =
\left( \begin{array}{cccc}
\sin \alpha & \cos \beta\\
\cos \alpha & \cos \beta\\
\end{array} \right)
\end{equation}

\begin{equation} \label{matrizR}
R =
\left( \begin{array}{cccc}
\sin \alpha & \sin \beta\\
\cos \alpha & \sin \beta\\
\end{array} \right)
\end{equation}

\begin{equation} \label{matrizS}
S =
\left( \begin{array}{cccc}
-\cos \alpha & \sin \beta\\
\sin \alpha & \sin \beta\\
\end{array} \right)
\end{equation}

\begin{equation} \label{matrizT}
T =
\left( \begin{array}{cccc}
\cos \alpha & \cos \beta\\
-\sin \alpha & \cos \beta\\
\end{array} \right)
\end{equation}

\newpage
\section{Bibliograf\'ia}

\end{document}
